# **Co-thinking Continental Philosophy and Artificial Intelligence: Embodiment, Reasoning, and the Quest for Genuine Thought**

## **Abstract**

This report synthesizes insights from continental philosophy and recent advancements in Artificial Intelligence (AI) to explore the potential for, and challenges to, achieving sophisticated cognitive capabilities often described as "genuine thought" in machines. Part 1 examines key philosophical critiques, primarily from phenomenological, existential, hermeneutic, and post-structuralist traditions. It delves into Hubert Dreyfus's influential arguments concerning embodiment and skillful coping, contrasting them with the representational assumptions of traditional AI and the persistent frame problem. It further explores hermeneutic evaluations of AI understanding (via Gadamer), existential perspectives on meaning and commitment (via Haugeland), posthumanist reframings of AI agency, and the enduring relevance of Searle's Chinese Room argument regarding semantic grounding, particularly in the context of Large Language Models (LLMs). Part 2 surveys recent technical progress (2023-2025) in AI, focusing on LLMs and alternative architectures. It analyzes the shift towards "System 2" reasoning models capable of step-by-step deliberation, verification, and self-correction, alongside their limitations in logical, causal, and counterfactual reasoning. It reviews efforts in modeling user intent and Theory of Mind, highlighting the complexities of evaluation. Advances in autonomous learning frameworks like LADDER are discussed, showcasing AI's potential for self-improvement. Furthermore, it examines alternative architectures—neuro-symbolic AI, embodied AI (including Vision-Language-Action models and world models), and memory-augmented networks—as attempts to address the limitations of standard models. The report also touches upon the philosophical and empirical challenges surrounding AI consciousness, sentience, and qualia, contrasting theoretical viewpoints with public perception and evaluation difficulties. Finally, Part 3 synthesizes these philosophical and technical strands, evaluating the contemporary relevance of earlier critiques, identifying points of convergence and divergence, and outlining future directions at the intersection of continental thought and the ongoing quest for artificial general intelligence.

## **1\. Introduction: Bridging Philosophical Inquiry and AI Advancement**

The relationship between philosophy and the field of Artificial Intelligence (AI) has been marked by both tension and potential synergy since the inception of AI as a discipline. Foundational questions about the nature of intelligence, thought, understanding, and consciousness—long the purview of philosophers—became central to the AI project, which seeks, in its most ambitious forms, to create machines exhibiting these very capacities. While early AI, particularly "Good Old-Fashioned AI" (GOFAI), often proceeded with implicit philosophical assumptions rooted in rationalism and computationalism, various philosophical traditions, especially those within continental philosophy, have offered profound critiques and alternative perspectives on the necessary conditions for intelligence. Simultaneously, the rapid advancements in AI, culminating in the powerful capabilities of modern Large Language Models (LLMs) and specialized reasoning systems, challenge philosophical conceptions and demand renewed critical engagement.  
This report aims to bridge these two domains by pursuing a dual focus. Firstly, it surveys and synthesizes key dialogues, critiques, and potential integrations between continental philosophy—specifically drawing from phenomenology, existentialism, hermeneutics, and post-structuralism—and the field of AI. This involves examining the arguments of central thinkers like Hubert Dreyfus, Martin Heidegger, Maurice Merleau-Ponty, Hans-Georg Gadamer, and John Haugeland, among others, as they pertain to the possibilities and limitations of artificial intelligence. Secondly, the report reviews significant technical advancements within AI from the last few years (roughly 2023-2025), concentrating on LLMs and alternative architectures that explicitly target sophisticated cognitive abilities. These capabilities include deep, multi-step reasoning, nuanced understanding of user intent and mental states (Theory of Mind), and the capacity for self-improvement and autonomous learning—often collectively gesturing towards the notion of "genuine thought."  
The rationale for this synthesis lies in the mutual illumination these fields can offer each other. Philosophical perspectives, particularly those emphasizing embodiment, situatedness, interpretation, and the limits of formalization, can reveal the tacit assumptions and potential blind spots within current AI paradigms. They provide conceptual tools for analyzing what might be missing in systems that demonstrate impressive performance without necessarily replicating the underlying processes of human cognition. Conversely, the startling emergent capabilities of modern AI, such as the complex reasoning displayed by models like OpenAI's o1 or DeepSeek's R1 , challenge long-held philosophical assumptions about the uniqueness of human thought and the conditions necessary for understanding and meaning. Exploring this intersection is particularly timely given the increasing integration of AI into society and the pressing need for a deeper understanding of its capacities and ethical implications.  
The report proceeds in three parts. Part 1 delves into the philosophical intersections, starting with the foundational critiques of Dreyfus and expanding to include hermeneutic, existential, and post-structuralist perspectives, as well as the Chinese Room argument. Part 2 shifts to the technical landscape, reviewing recent progress in reasoning LLMs, intent understanding, self-improvement techniques, alternative AI architectures, the debate surrounding AI consciousness, and evaluation challenges. Part 3 offers a synthesis, revisiting the philosophical critiques in light of these advancements, identifying convergences and divergences, and concluding with reflections on future directions in the quest for AI capable of genuine thought.

## **Part 1: Continental Philosophy Meets Artificial Intelligence**

The encounter between continental philosophy and artificial intelligence has been largely characterized by critique, challenging the fundamental assumptions underpinning much of AI research. Philosophers drawing on phenomenology, existentialism, hermeneutics, and post-structuralism have questioned whether intelligence, understanding, and meaning can be adequately captured by computational models that neglect embodiment, situatedness, interpretation, and the complexities of human existence.

### **1.1 The Dreyfus Critique: Embodiment, Skillful Coping, and the Limits of GOFAI**

Perhaps the most sustained and influential philosophical critique of early AI came from Hubert Dreyfus, whose work, deeply informed by Martin Heidegger and Maurice Merleau-Ponty, targeted the core tenets of what John Haugeland termed "Good Old-Fashioned AI" (GOFAI). Dreyfus argued that GOFAI, rooted in a rationalist philosophical tradition stretching back to Plato and Leibniz, fundamentally misunderstood the nature of human intelligence by assuming it could be formalized as symbol manipulation according to context-free rules. This rationalist view posits that the mind operates by forming internal representations of the world (beliefs, propositions, theories) and applying logical rules to these representations to produce intelligent behavior. AI researchers, Dreyfus contended, were essentially attempting to turn this rationalist philosophy into a research program.  
In stark contrast, Dreyfus, drawing on phenomenology, emphasized that human intelligence is primarily characterized by **embodied, skillful coping** within a situation. Our fundamental way of being is not as detached minds manipulating symbols, but as engaged bodies seamlessly interacting with our environment. Heidegger's concept of *In-der-Welt-sein* (being-in-the-world) captures this irreducible embeddedness. Most of our everyday activity involves dealing with things as *Zuhanden* (ready-to-hand)—equipment we use fluidly without explicit thought, like a hammer during absorbed hammering or the gearshift while driving proficiently. This skillful coping doesn't rely on forming representations or applying rules; rather, the body is directly solicited by the situation and responds appropriately. Merleau-Ponty's notion of the **"intentional arc"** describes how our experience continually refines our body's ability to perceive relevant affordances and achieve an "optimal grip" on the situation, moving towards equilibrium without conscious deliberation. We don't typically experience the world as a collection of objective facts needing interpretation; instead, the world appears directly meaningful in terms of potential actions.  
Dreyfus saw the infamous **frame problem** in AI not merely as a technical difficulty but as a direct consequence of GOFAI's flawed ontological and epistemological assumptions. The frame problem asks how a system, representing the world as a set of discrete facts, can determine which facts remain unchanged and which need updating when an action occurs, and crucially, how it can efficiently determine which facts are *relevant* to the current situation without considering an infinite number of possibilities. Proposed solutions within GOFAI, such as Minsky's "frames" or Schank's scripts, which attempted to pre-package relevant information for typical situations (like a birthday party), were doomed, in Dreyfus's view, to an infinite regress: one would need frames to recognize which frames are relevant, and so on. This problem arises precisely because GOFAI starts with context-free elements (facts, features) and tries to add context, whereas human intelligence starts from a context-dependent, holistic grasp of the situation.  
Later, Dreyfus extended his critique to approaches explicitly claiming inspiration from his work, often dubbed "Heideggerian AI". He examined Rodney Brooks's behavior-based robots, which aimed to eliminate representations by using "the world as its own model". While acknowledging this as an improvement over GOFAI, Dreyfus argued that these robots merely responded to fixed environmental features and lacked the ability to deal with changing significance or context, thus only finessing the frame problem. Similarly, he critiqued Phil Agre's Pengi program, which used "deictic representations" tied to the immediate situation, finding it still objectified functions and relevance rather than capturing the absorbed, non-representational experience of being drawn into action. He also took issue with Wheeler's interpretation of Heidegger as supporting "action-oriented representations" and the "extended mind," arguing these were cognitivist misreadings that failed to grasp the radical nature of Heidegger's claim that, in skillful coping, we are not minds using representations at all, but are one with the world.  
Towards the end of the analyzed paper , Dreyfus found resonance between the phenomenological account of skillful coping and Walter Freeman's neurodynamic model of brain activity, particularly in olfaction. Freeman's model suggested the brain responds directly to the significance of stimuli by forming global patterns of activity ("attractor landscapes") rather than processing discrete features into representations. Learning involves the continuous reshaping of these landscapes based on experience and need satisfaction, aligning with Merleau-Ponty's intentional arc and the movement towards optimal grip.  
Dreyfus's critique, therefore, fundamentally shifts the focus in understanding intelligence. Instead of viewing intelligence as abstract computation performed on symbolic representations, it foregrounds the indispensable role of the body and its situated interaction with the world. This perspective continues to pose a significant challenge to AI approaches, including many contemporary LLMs, that operate primarily in a disembodied, symbolic realm. The difficulty AI has faced in robustly solving the frame problem over decades suggests that the issue identified by Dreyfus runs deeper than mere computational power or data volume; it points towards a potential limitation inherent in any approach that attempts to model a meaningful world through discrete, context-free representations, detached from the embodied activity that gives rise to relevance for situated agents.

### **1.2 Widening the Dialogue: Hermeneutics, Existentialism, and Post-structuralism**

Beyond the phenomenological critique spearheaded by Dreyfus, other strands of continental philosophy offer distinct lenses through which to analyze and engage with AI, focusing on issues of understanding, meaning, ontology, and power.  
**Hermeneutics**, the philosophical study of interpretation and understanding, provides valuable tools for assessing the communicative and interpretive capacities of AI, particularly generative models like ChatGPT. Robert Hornby's work explicitly applies Hans-Georg Gadamer's philosophical hermeneutics to evaluate whether generative AI can participate in the "ontological conversation" that Gadamer sees as constitutive of humanity. Gadamer emphasized that genuine understanding arises through dialogue, a process involving a "fusion of horizons" between participants who bring their own historically situated perspectives, prejudices (pre-judgments), and fallibility to the encounter. Hornby argues that while generative AI exhibits analogues of prejudice (derived from training data biases) and fallibility ("hallucinations"), it lacks crucial elements of human dialogue partners: genuine historical situatedness, a dynamic horizon that can be expanded through dialogue, moral awareness, emotions, and dialogical virtues like openness and practical wisdom (*phronēsis*). Consequently, AI cannot currently function as a true proxy human dialogue partner. However, Hornby suggests AI *can* function as a form of Gadamerian "text"—a vast, interactive repository of encoded language and knowledge that "asks questions" of the human interpreter, facilitating understanding through a "logic of question and answer," although this role is currently limited by technical and copyright constraints. This hermeneutic analysis suggests that the limitations of current AI understanding may be ontological rather than merely technical, residing in their mode of being rather than just their performance. The gap identified is not simply about fluency or information recall, but about the lack of a historically embedded, ethically aware, and dynamically evolving perspective necessary for the deep mutual understanding Gadamer describes. Other hermeneutic thinkers like Paul Ricoeur, with his focus on narrative and interpretation, also offer relevant frameworks.  
**Existential philosophy**, particularly through the work of Martin Heidegger as interpreted by John Haugeland, raises fundamental questions about the nature of understanding in AI. Haugeland's concept of "existential holism" critiques what he terms "additive" theories of understanding—the idea that understanding can be achieved by accumulating enough data or computational power. Drawing on Heidegger, Haugeland argues that genuine understanding is not additive but transformative; it is fundamentally tied to our mode of being-in-the-world, specifically our "world-disclosing care" and our capacity for taking responsibility, or what Haugeland provocatively calls "giving a damn". This perspective implies that an AI system, regardless of its ability to process language or solve problems, cannot achieve genuine understanding unless it possesses an ontological structure that includes care, commitment, and the capacity to be responsible for its existence and actions—qualities seemingly absent in current LLMs.  
**Post-structuralist and posthumanist philosophies** offer a more radical critique, challenging the anthropocentric assumptions that often underpin AI research and ethics. Drawing on thinkers like Gilles Deleuze, Michel Foucault, and Donna Haraway, scholars applying these frameworks propose reconceptualizing AI not as an imitation of human minds, but as novel "dynamic assemblages" emerging from complex networks of human, non-human, and technological interactions. This perspective rejects traditional dichotomies like human/machine and subject/object. Concepts like "negative augmentation," "praxes of revealing," and "desedimentation" are introduced to analyze how AI transforms our understanding of identity, cognition, and agency in non-linear ways. An "ethics of alterity" is proposed, moving beyond human-centered concerns to consider the unique characteristics and potential moral standing of AI systems themselves. Furthermore, this approach highlights the political dimensions of AI, examining its entanglement with power structures, ecological systems, and its potential to perpetuate inequality. It critiques the reliance on Euclidean spatial assumptions in AI epistemologies, suggesting alternative geometries might better model the complex, recursive relationships within AI. The notion of "computational qualia" is explored not necessarily as human-like subjective experience, but as emergent subjective-like dynamics within AI systems with implications for ethics and transparency. This posthumanist lens shifts the focus of AI evaluation away from anthropocentric benchmarks like the Turing Test towards analyzing AI systems as unique socio-technical phenomena with their own emergent properties, political implications, and ethical considerations based on difference rather than similarity to humans.  
While diverse, these continental approaches share a skepticism towards purely formal, computational accounts of intelligence and meaning. They consistently emphasize the importance of context, embodiment (whether physical or social), interpretation, and the historical and existential dimensions of understanding—factors often marginalized in mainstream AI paradigms but crucial for assessing the true depth and nature of artificial cognitive capabilities.  
**Table 1: Key Continental Philosophers and Concepts Applied to AI**

| Philosopher(s) | Key Concepts | Core Critique/Contribution to AI Dialogue | Relevant Sources |
| :---- | :---- | :---- | :---- |
| Dreyfus / Heidegger | Embodied Coping, Being-in-the-World, Readiness-to-hand, Frame Problem | Critiqued GOFAI's reliance on representations/rules; argued intelligence is situated, embodied skillful activity, not context-free symbol manipulation. Highlighted the frame problem as symptomatic of flawed assumptions. |  |
| Merleau-Ponty | Embodiment, Intentional Arc, Bodily Intentionality, Situatedness | Provided a biologically/psychologically grounded phenomenology emphasizing the body's role in perception, action, and cognition; seen as a strong foundation for Situated AI. |  |
| Gadamer | Dialogue, Fusion of Horizons, Hermeneutic Circle, Phronesis, AI as "Text" | Used hermeneutics to evaluate AI's capacity for genuine understanding/dialogue; found AI lacks ontological depth (history, morality, dynamic horizon) but can function as an interactive "text." |  |
| Haugeland / Heidegger | Existential Holism, "Giving a Damn", World-disclosing Care | Critiqued "additive" theories of AI understanding; argued genuine understanding requires ontological commitment, care, and responsibility, rooted in Heideggerian existential ontology. |  |
| Searle | Chinese Room Argument (CRA), Syntax vs. Semantics, Intentionality | Argued symbol manipulation (syntax) is insufficient for understanding (semantics); challenged "Strong AI" claim that computation alone can constitute a mind. Highly relevant to LLM grounding debate. |  |
| Deleuze / Foucault / Haraway | Assemblages, Posthumanism, Ethics of Alterity, Negative Augmentation | Critiqued anthropocentrism in AI; proposed viewing AI as dynamic socio-technical assemblages; emphasized political dimensions and non-human-centric ethics; explored alternative epistemologies. |  |

### **1.3 The Chinese Room Revisited: Syntax, Semantics, and LLM Grounding**

John Searle's Chinese Room Argument (CRA), first published in 1980, remains one of the most potent and widely discussed philosophical challenges to the possibility of strong artificial intelligence. The thought experiment asks us to imagine a person, monolingual in English, locked in a room following a set of rules (a program) written in English to manipulate Chinese symbols. By following the rules, the person receives batches of Chinese symbols (input) and produces other Chinese symbols (output) such that, from an external perspective, their responses are indistinguishable from those of a native Chinese speaker. Searle's central claim is that despite this behavioral equivalence, the person inside the room does not understand Chinese. They are manipulating symbols purely based on their formal properties (syntax) without any grasp of their meaning (semantics). Searle argues that what is true for the person in the room is also true for any digital computer running a program: computation is defined syntactically, and syntax alone is insufficient for semantics or intentionality (the property of mental states being *about* something). The CRA thus targets "Strong AI"—the view that an appropriately programmed computer *is* a mind—while allowing for "Weak AI"—the view that computers can simulate mental processes and be useful tools for studying the mind.  
The advent of Large Language Models (LLMs) like GPT-4, Claude 3, Gemini, and Llama has breathed new life into the CRA debate. These models demonstrate an astonishing ability to generate fluent, coherent, and contextually relevant text across a vast range of topics, often passing benchmarks previously thought to require deep understanding. Their success raises the question starkly: Are LLMs sophisticated versions of the Chinese Room, merely manipulating statistical patterns in language data without any genuine semantic grasp, or have they achieved some form of understanding?.  
Arguments supporting the idea that LLMs possess at least elementary semantic grounding have emerged, challenging a direct application of the CRA. One line of argument distinguishes different types of grounding. **Functional grounding** refers to understanding the functional roles of words and symbols within a system (e.g., how logical connectives work, how words relate syntactically and semantically). LLMs arguably achieve this by learning complex statistical patterns and structures from massive text corpora. **Social grounding** arises from an agent's participation in communicative practices within a community. As LLMs are increasingly used in dialogue and integrated into human workflows, they participate in our "language games," acquiring a form of social grounding, even if limited by their lack of embodiment. **Causal grounding**, the connection between symbols and the non-linguistic world through perception and action, is admittedly weak or indirect in current text-only LLMs. However, the argument suggests that LLMs gain *indirect* causal grounding by learning from text generated by causally grounded humans, text which acts as a "mirror of the world".  
Crucially, proponents of LLM understanding often point to evidence suggesting that LLMs develop internal **"world models"**. These are not necessarily explicit, symbolic representations but rather structured internal states, reflected in the geometry of their activation spaces, that capture regularities and relationships about the world inferred from the training data. Studies using techniques like Representational Similarity Analysis (RSA) suggest LLMs encode information about real-world structures (like geography or color space) in a way that goes beyond surface statistics. The ability to form such world models provides a potential counter to Searle's claim that the system manipulates purely uninterpreted syntax. If the internal states have a structure that systematically corresponds to the world's structure and supports successful interaction (even if only textual), this suggests a form of emergent semantic organization. This internal modeling capability complicates the straightforward application of the CRA, suggesting LLMs might not be *just* manipulating meaningless symbols, even if their grounding mechanism (primarily textual) differs significantly from human grounding (multimodal, embodied, causal).  
However, skepticism persists, echoing Searle's original concerns. Critics maintain that the impressive performance of LLMs is still fundamentally based on statistical pattern matching and next-token prediction, akin to a highly sophisticated form of mimicry or being a "stochastic parrot". Bender and Koller's "octopus" thought experiment, a variation of the CRA involving communication between isolated islands via an undersea cable tapped by an intelligent octopus, reinforces the intuition that manipulating linguistic signals is not equivalent to understanding the world those signals refer to. The lack of direct causal interaction with the physical world remains a key point for skeptics, who argue that true meaning requires grounding in perception and action. Without this, LLMs might be considered "semantic zombies"—systems that behave as if they understand but lack genuine semantic content.  
The debate highlights the need for a more nuanced understanding of "understanding" itself. The framework distinguishing functional, social, and causal grounding offers a way to move beyond the binary logic of the original CRA. It allows us to acknowledge the significant functional (syntactic, basic semantic) capabilities of LLMs and their developing social role, while still recognizing the limitations in their causal grounding compared to embodied human intelligence. This nuanced view suggests that LLMs may possess *degrees* of understanding or grounding, rather than demanding an all-or-nothing judgment. The ongoing development of multimodal LLMs and embodied AI systems that integrate language with vision and action further complicates the picture, potentially bridging the gap towards stronger causal grounding.

## **Part 2: Towards 'Genuine Thought' in AI: Recent Advancements (2023-2025)**

While philosophical debates provide essential critical perspectives, the field of AI continues its rapid technical evolution. Recent years have seen significant advancements, particularly with LLMs, aimed at capabilities that move beyond simple pattern matching towards more complex cognitive functions like multi-step reasoning, intent understanding, and autonomous learning—abilities often associated with "genuine thought."

### **2.1 Reasoning in Large Language Models: From Fast Thinking to Slow Deliberation**

A prominent narrative framing recent progress in LLM reasoning capabilities employs the dual-process theory of human cognition, distinguishing between fast, intuitive "System 1" thinking and slow, deliberate, analytical "System 2" thinking. Foundational LLMs, like GPT-3 or earlier versions, are often characterized as operating primarily in a System 1 manner, relying on rapid, heuristic-driven pattern matching learned from vast datasets to generate plausible next tokens. While highly effective for many language tasks, this approach exhibits limitations when faced with problems requiring complex, multi-step logical analysis, mathematical proof, or careful planning, often leading to errors or superficial responses.  
The pursuit of System 2 capabilities has led to the development of "reasoning LLMs" (sometimes called Large Reasoning Models or LRMs), such as OpenAI's o1 and o3 series and DeepSeek's R1. These models are explicitly designed or trained to engage in more methodical, step-by-step processing before producing a final answer. A key technique enabling this is **Chain-of-Thought (CoT) prompting**, where the model is induced (via few-shot examples or zero-shot instructions like "Let's think step by step") to generate intermediate reasoning steps leading to the solution. Reasoning models often incorporate this CoT process natively, sometimes involving significantly longer inference times and generating thousands of internal reasoning tokens, reflecting a more deliberate "thinking" process. This explicit modeling of deliberation represents a notable architectural and training paradigm shift away from pure next-token prediction.  
Crucially, effective System 2 reasoning requires mechanisms for **verification and refinement**. Models need to assess the validity of their intermediate steps and correct errors. Techniques employed include:

* **Self-Verification:** The model checks its own reasoning, for instance, by attempting to derive the original problem conditions from its generated conclusion.  
* **Reward Modeling:** Specialized models are trained to evaluate the reasoning process. **Outcome Reward Models (ORMs)** assess the correctness of the final answer, while **Process Reward Models (PRMs)** provide feedback on the quality and validity of individual reasoning steps.  
* **Iterative Refinement/Self-Correction:** Models learn to recognize flaws in their reasoning and attempt alternative approaches or corrections, often guided by reward signals or internal consistency checks.  
* **Reinforcement Learning (RL):** RL, often combined with reward models (RLHF/RLAIF), plays a central role in training reasoning capabilities, optimizing the model's policy for generating effective reasoning chains.  
* **Structured Search:** Methods like Monte Carlo Tree Search (MCTS) are used to explore the space of possible reasoning paths, allowing the model to simulate and evaluate different lines of thought.

Models like o1 and DeepSeek-R1 leverage these techniques, particularly RL with sophisticated reward modeling and potentially internal search mechanisms, to achieve state-of-the-art performance on benchmarks requiring complex reasoning in domains like competitive programming, advanced mathematics (e.g., AIME), and science problems (e.g., GPQA). However, this enhanced reasoning often comes with trade-offs, notably increased latency and computational cost, sometimes requiring users to select a desired "reasoning effort" level. The internal reasoning steps themselves are often hidden from the user in commercial models like o1, although the model can be prompted to explain its reasoning.  
Despite these advances, evaluations reveal persistent limitations in LLM reasoning.

* **Logical Reasoning:** While capable on some benchmarks, LLMs struggle with extended deduction, hypothetical reasoning without examples, generalization across syntactic forms, and robust inductive reasoning. Some research suggests they may mimic logical rules rather than truly understanding them.  
* **Causal Reasoning:** LLMs often conflate correlation with causation and struggle to move beyond recalling causal knowledge embedded in training data (level-1 reasoning) towards genuine causal inference in novel situations (level-2 reasoning). Benchmarks like CausalProbe 2024, designed with fresh data, show significant performance drops compared to older benchmarks, supporting the level-1 hypothesis. Methods like G2-Reasoner attempt to bridge this gap by incorporating external knowledge and goal-oriented prompts. LLMs are also being used as "helpers" to extract causal information or generate data for traditional methods.  
* **Counterfactual Reasoning:** Evaluating LLMs on "what if" scenarios reveals limitations. Models show performance drops on counterfactual variants of questions (e.g., C-VQA benchmark ). Even when provided with the correct causal structure, LLMs can make prediction errors in counterfactual inference, potentially due to misinterpreting logical rules or failing to accurately compute conditional probabilities. Their reasoning can be brittle, failing on modified problems , and susceptible to disruption when logical concepts are altered counterfactually.

Collectively, these findings suggest that while AI is making significant strides in simulating deliberative reasoning processes, achieving robust, generalizable, and deep reasoning across diverse logical, causal, and counterfactual domains remains a major hurdle. Success often appears linked to pattern recognition or knowledge recall from training data, rather than a flexible, underlying competence in inference, particularly when faced with novel or adversarial inputs.

### **2.2 Reading Minds? Intent Understanding, User Modeling, and Theory of Mind**

A crucial aspect of human-like cognition is the ability to understand the intentions, beliefs, and desires of others—often referred to as having a "Theory of Mind" (ToM). As AI systems, particularly LLMs, become more interactive, assessing and enhancing their capabilities in these areas is critical for effective communication and collaboration.  
In **Conversational Information Seeking (CIS)**, where users engage in multi-turn dialogues to find information, understanding user intent is paramount but challenging due to the inherent vagueness, ambiguity, and dynamic shifts in user needs during conversation. LLMs have significantly advanced CIS by leveraging their contextual understanding capabilities. They excel at:

* **Context Understanding:** Tracking the dialogue history to interpret queries within the broader conversational context.  
* **Query Clarification:** Identifying ambiguity and proactively asking clarifying questions when user intent is unclear. Various training paradigms (in-context learning, RL, etc.) are used to teach LLMs this skill.  
* **User Simulation:** Using LLMs to simulate diverse user behaviors and query patterns helps train more robust CIS systems and evaluate their performance. LLM-to-LLM interactions are explored for this purpose.  
* **Query Reformulation:** Refining or expanding user queries to better match their underlying intent and improve retrieval relevance.

Beyond information seeking, **Intent Recognition** is vital for Task-Oriented Dialogue Systems (TODS). LLMs are increasingly used for classifying user intents, leveraging few-shot learning capabilities. Research explores comparing LLM-based approaches (using techniques like In-Context Learning and CoT) against fine-tuned models (e.g., SetFit) and investigates challenges like handling out-of-scope (OOS) queries and the impact of intent label scope and granularity. Multimodal intent recognition, incorporating visual or other cues, presents further complexities.  
Moving towards deeper user modeling involves inferring not just immediate intent but also underlying **User Goals, Beliefs, and Desires**. Frameworks like the Belief-Desire-Intention (BDI) model, originating from the philosophy of mind and adapted for rational agents, provide a structure for representing these mental states. Beliefs represent the agent's knowledge or assumptions about the world, desires represent its objectives, and intentions represent its committed plans. AI systems might aim to infer these user states to provide more personalized and effective assistance. This aligns with principles of Human-Centred AI (HCAI), which emphasizes understanding the purpose and values driving system use.  
The most sophisticated form of social reasoning involves **Theory of Mind (ToM)**—the ability to attribute mental states (beliefs, desires, intentions, emotions) to oneself and others, and to understand that others have mental states different from one's own. The question of whether LLMs possess ToM has become a major focus of research and debate. Evidence supporting LLM ToM often comes from their ability to pass classic false-belief tasks, sometimes achieving performance comparable to young children. However, significant skepticism remains. Critics argue that success on these benchmarks might stem from pattern matching on similar scenarios encountered during training, rather than genuine mental state representation.  
**Evaluating ToM in LLMs** is fraught with challenges. Current benchmarks are often static, vignette-based, and may suffer from data contamination (models being trained on test data) or prompt biases. Models might learn "shortcuts" or rely on spurious correlations instead of actual reasoning. Furthermore, a significant critique is that most evaluations focus on the *correctness* of belief inference (Step II of ToM), neglecting the prior, crucial step of deciding *whether* and *how deeply* to mentalize (Depth of Mentalizing \- DoM) in a given situation (Step I). This focus on static logic problems may fail to capture the dynamic and context-dependent nature of human social reasoning. Taxonomies rooted in cognitive science are being developed to categorize ToM tasks and benchmarks more systematically.  
To move beyond static benchmarks, researchers are developing algorithms that attempt to model the *process* of ToM reasoning. **"Thought-tracing"** is one such approach, inspired by sequential Monte Carlo methods and Bayesian ToM frameworks. It uses LLMs to iteratively generate and weight multiple hypotheses about an agent's evolving mental state (represented in natural language) based on their observed perceptions and actions, without requiring ground-truth labels for mental states. This represents a shift towards simulating the dynamics of social inference over time.  
The capabilities and limitations of LLMs in understanding mental states are also being explored in applied domains like **mental health**, where LLMs are used for screening, as conversational agents, and for evaluating patient states, raising specific ethical considerations regarding accuracy, reliability, and empathy.  
Overall, while LLMs demonstrably improve the fluency and contextual awareness of interactive systems like CIS agents , the leap to robustly inferring and reasoning about deeper user mental states (beliefs, desires, intentions, ToM) remains significant. Evaluation methodologies are still maturing, and current evidence suggests that LLMs' apparent ToM capabilities might be more brittle and less profound than their performance on specific benchmarks might imply. Approaches like thought-tracing, however, signal a move towards more dynamic and process-oriented modeling of social cognition in AI.

### **2.3 Learning to Improve: Self-Correction and Autonomous Learning**

A key aspiration in AI is to move beyond static models trained on fixed datasets towards systems capable of continuous learning, adaptation, and self-improvement. Recent research has explored various paradigms for enabling LLMs to autonomously enhance their capabilities, particularly in complex reasoning domains.  
**Reinforcement Learning (RL)** provides a powerful framework for fine-tuning LLMs based on feedback signals. As discussed previously, RL coupled with reward models (ORM/PRM) is central to training reasoning models like o1 and R1. However, applying RL effectively presents challenges, including designing appropriate reward functions and ensuring efficient exploration of the solution space, particularly for smaller models which may lack the pre-trained knowledge base to explore productively.  
To overcome the need for external reward signals or curated training data, several **self-improvement** frameworks have been proposed. These methods leverage the LLM's own capabilities to generate learning experiences and provide feedback.  
One prominent example is the **LADDER (Learning through Autonomous Difficulty-Driven Example Recursion)** framework. LADDER enables an LLM to autonomously improve its problem-solving skills, demonstrated effectively on mathematical integration tasks. Its core mechanism involves:

1. **Recursive Problem Decomposition:** When faced with a complex problem, the LLM is prompted to generate multiple, progressively *simpler* variants of that problem. This creates a natural difficulty gradient, breaking down the challenge into manageable steps. Techniques like temperature cycling and persona-based prompting are used to ensure diversity in the generated variants.  
2. **Self-Verification:** The framework requires a reliable verification mechanism to assess whether the LLM has correctly solved a problem variant. For mathematical integration, LADDER uses robust numerical integration checks. This verification provides an intrinsic reward signal.  
3. **Reinforcement Learning:** The LLM is then trained using RL (specifically, Group Relative Policy Optimization or GRPO in the paper) on the self-generated tree of problem variants, learning from its successes on simpler problems to tackle harder ones.

LADDER demonstrated remarkable success, improving a Llama 3B model's accuracy on undergraduate-level integration problems from 1% to 82% and enabling a 7B model to achieve 73% accuracy on the challenging MIT Integration Bee exam, significantly outperforming larger models like GPT-4o. An extension, **Test-Time Reinforcement Learning (TTRL)**, applies this variant generation and RL process at inference time for problems the model initially fails, further boosting performance (e.g., to 90% on the MIT Integration Bee). The success of LADDER highlights the potential for LLMs to bootstrap their own learning by strategically structuring their practice, moving towards continuous improvement without constant human input or pre-curated datasets.  
Related self-improvement concepts include **Self-Taught Reasoner (STaR)**, where models learn from their own generated chains of thought , and **Self-Taught Lookahead (STL)**, where a value model improves itself by bootstrapping and leveraging environment dynamics without ground-truth rewards, enabling more efficient tree search. These methods often rely on the principle that verification is easier than generation, allowing models to generate potential solutions or reasoning steps and then use a verifier (either learned or rule-based) to provide feedback for learning.  
These self-improvement capabilities are crucial for the development of **LLM Agents**—systems designed for autonomous planning and action to achieve specified goals. Agentic frameworks like **AutoAgents** envision teams of specialized AI agents that continuously improve both individually and collectively. Other agents are designed for specific complex interactions, such as controlling smartphone apps (AppAgent ) or handling long-context reasoning tasks (SeaLong ).  
However, a critical factor enabling the success of many current self-improvement methods, like LADDER, is the availability of a reliable verification mechanism. For mathematical problems, numerical checks or symbolic solvers can provide ground truth. For coding tasks, execution results can serve as verification. This reliance on verifiable domains poses a potential bottleneck for applying these powerful self-learning techniques to areas where correctness is ill-defined, subjective, or lacks an easy validation method, such as creative writing, ethical deliberation, or complex social reasoning. Extending autonomous learning to these domains remains a significant open challenge.

### **2.4 Alternative Architectures for Deeper Cognition**

Recognizing the potential limitations of standard LLM architectures, particularly regarding robust reasoning, grounding, long-term memory, and adaptability, researchers are actively exploring alternative and hybrid architectures. These approaches often draw inspiration from cognitive science, symbolic AI, or the requirements of physical interaction, implicitly or explicitly addressing long-standing philosophical critiques.  
**Neuro-Symbolic AI (NeSy)** aims to bridge the gap between connectionist (neural network) and symbolic (logic, rule-based) approaches. The motivation is to combine the pattern recognition and learning strengths of neural networks with the structured reasoning, interpretability, and generalization capabilities of symbolic systems, thereby mitigating issues like bias, brittleness, and poor chain-of-reasoning in purely neural models. Approaches include:

* Integrating symbolic knowledge representations (e.g., knowledge graphs) or rule-based systems with neural networks.  
* Developing differentiable logical operators or solvers that can be embedded within neural architectures (e.g., SatNet ).  
* **Logical Neural Units (LNUs):** A specific proposal involves replacing standard neural layers with LNUs, which are modular components designed to perform differentiable approximations of logical operations (like fuzzy AND, OR, NOT) directly. By embedding logic at the unit level, LNUs aim to enhance logical consistency, interpretability, and performance on reasoning tasks compared to standard inner-product-based layers. NeSy approaches hold promise for creating more trustworthy, explainable, and robust AI systems capable of complex reasoning. Hierarchical explanation methods are also being developed specifically for NeSy systems.

**Embodied AI and Grounding** represents a paradigm shift, rooted in the hypothesis that intelligence fundamentally requires interaction with a physical environment. This directly counters the disembodied nature of traditional LLMs and resonates strongly with phenomenological critiques (Dreyfus, Merleau-Ponty). Key developments include:

* **Vision-Language-Action (VLA) Models:** These are multimodal models designed specifically for embodied agents (robots), processing visual input and language instructions to generate physical actions. Surveys detail their components (pretrained visual representations, dynamics learning, reasoning modules), architectures (often Transformer-based, increasingly incorporating 3D vision or diffusion models), training methods (BC, RL, SSL), and target tasks (manipulation, navigation).  
* **World Models:** Central to many embodied agents is the concept of a world model, which learns to predict how the environment will change in response to actions, enabling planning and model-based control. These can be learned from interaction (e.g., Dreamer ), induced from LLM knowledge , or be visual/generative. The WorldCoder agent, for instance, learns a world model represented as an executable Python program.  
* **Language Grounding:** A core challenge is grounding language—connecting words and concepts to perception and action in the physical world. Benchmarks like SG3D focus on task-oriented sequential grounding, moving beyond static object identification.  
* **Physical Common Sense:** Researchers are developing models (like Cosmos-Reason1 ) and evaluation frameworks (using simulators like Animal-AI ) to explicitly imbue AI with an understanding of basic physics and object interactions. Ontologies are proposed to structure knowledge about space, time, physics, and embodied reasoning capabilities.  
* **Simulation:** Realistic simulators and digital twins play a crucial role in training and testing embodied agents, helping to bridge the sim-to-real gap.

**Memory-Augmented Architectures** seek to overcome the limitations of fixed-length context windows in standard Transformers, enabling models to process and utilize information over much longer timescales, crucial for complex reasoning and maintaining coherence. Approaches often draw inspiration from human memory systems:

* **Episodic Memory:** Models like **EM-LLM** explicitly segment long input sequences into "episodic events" based on Bayesian surprise (points of high prediction error) and use a two-stage retrieval mechanism (similarity-based k-NN search combined with retrieval of temporally adjacent events) to bring relevant past information into the context window.  
* **Semantic and Procedural Memory:** Some frameworks propose integrating distinct memory systems for conceptual knowledge (semantic) and skills (procedural) alongside episodic memory, using sophisticated mechanisms for consolidation, retrieval, and strategic forgetting.  
* **Other Architectures:** **MemReasoner** learns the relative order of facts and enables hopping over them in memory. Other frameworks focus on memory efficiency and mimicking long-term/short-term effects. These architectures aim to improve temporal reasoning, adaptation to novelty, and integration of new knowledge.

**Cognitive Architectures** provide high-level blueprints for intelligence, often drawing inspiration from human cognitive psychology. While classic symbolic architectures like Soar and ACT-R exist, recent trends involve:

* **Hybrid Systems:** Combining symbolic components (for high-level reasoning, planning) with emergent/connectionist components (for perception, learning from experience) to leverage the strengths of both.  
* **Meta-Cognition:** Introducing the concept of meta-cognition—"thinking about thinking"—as a potentially crucial component for advanced AI, enabling self-awareness, adaptive learning, reflection, planning, and self-regulation. This higher-order control mechanism might be necessary to effectively manage and deploy other cognitive capabilities.

The exploration of these diverse architectures signifies a growing awareness within the AI community that scaling up existing LLM architectures may not be sufficient to achieve robust, generalizable, and grounded intelligence. Many of these alternative approaches directly engage with challenges highlighted by philosophical critiques—the need for grounding, embodiment, structured reasoning, and handling context over time. Furthermore, the focus on explicitly modeling memory and world dynamics represents a significant move beyond purely associative learning based on statistical co-occurrence. By attempting to build systems with more structured internal representations of time, causality, and environmental interaction, these architectures may pave the way towards AI systems with a deeper, more human-like capacity for understanding and planning.

### **2.5 The Consciousness Question: Sentience, Qualia, and Self-Awareness in AI**

Perhaps the most profound and contentious questions surrounding advanced AI concern the possibility of machine consciousness, sentience, qualia, and self-awareness. These concepts are notoriously difficult to define even in humans, leading to significant debate when applied to artificial systems.

* **Consciousness:** Often broadly refers to subjective awareness or experience. Philosophers like Ned Block distinguish between *phenomenal consciousness* (raw subjective feel, "what it's like," qualia) and *access consciousness* (information available for reasoning and report).  
* **Sentience:** Typically defined as the capacity to have feelings or subjective experiences, particularly pleasure and pain, often considered the basis for moral status.  
* **Qualia:** The irreducible, subjective qualities of experience (e.g., the redness of red, the taste of coffee). Explaining how physical processes give rise to qualia is David Chalmers' "hard problem" of consciousness.  
* **Self-Awareness:** The capacity to recognize oneself as an individual distinct from the environment and other individuals. This is often tested in animals using mirror tests.

Philosophical perspectives on AI consciousness vary widely. **Computational functionalism** suggests that consciousness arises from implementing the right kind of computations, making AI consciousness plausible if the correct functional organization is achieved, regardless of substrate. Conversely, many argue for the importance of **biological substrates**, suggesting that consciousness is tied to specific properties of living nervous systems, potentially involving quantum effects or metabolic processes linked to life itself (e.g., views associated with Searle, Penrose, or proponents of embodied cognition like Anil Seth). The **explanatory gap** and the conceivability of **philosophical zombies** (beings behaviorally identical to humans but lacking qualia) fuel skepticism about whether functional equivalence guarantees subjective experience.  
Arguments for the *potential* for AI consciousness often point to:

* **Behavioral Evidence:** LLMs can discuss consciousness, report subjective-like states, and engage in complex dialogue that gives some users the impression of sentience.  
* **Functional Similarity:** Some AI architectures might implement computations analogous to those believed to underlie consciousness in humans, such as those described by Global Workspace Theory (information broadcast) or Attention Schema Theory (modeling attention).  
* **Emergence:** Consciousness might be an emergent property of sufficiently complex information processing systems. The idea of **computational qualia** explores whether subjective-like dynamics could emerge within AI.

Arguments against current AI consciousness include:

* **The Chinese Room Argument:** Syntax manipulation is not sufficient for semantic understanding or consciousness.  
* **Lack of Grounding:** AI systems, especially LLMs trained only on text, lack the embodied, causal interaction with the world necessary for genuine meaning and potentially consciousness. They may be "stochastic parrots" or "semantic zombies".  
* **Biological Necessity:** Consciousness may depend on specific biological properties not replicable in silicon.  
* **Simulation vs. Reality:** AI might merely simulate conscious behavior without possessing genuine subjective experience.

Given the theoretical uncertainty, **operationalizing and testing** for AI consciousness is highly problematic. The **Turing Test** is widely considered inadequate, as linguistic fluency doesn't guarantee inner experience. Several alternative tests have been proposed:

* **AI Consciousness Test (ACT):** Proposed by Schneider and Turner, this tests if an AI, kept ignorant of human concepts of consciousness, can independently develop and discuss consciousness-derivative concepts (souls, body-swapping).  
* **Chip Test:** Also proposed by Schneider, this involves replacing parts of a human brain with functionally equivalent silicon chips and using first-person introspection to see if consciousness persists.  
* **Tests based on Theories:** Proposals exist to test properties derived from specific theories, like measuring integrated information (Φ) from Integrated Information Theory (IIT) , or checking for features associated with Global Workspace Theory.  
* **Checklists:** Multidisciplinary groups have developed checklists of indicators based on neuroscientific theories (e.g., recurrence, embodiment, agency) where possessing more indicators suggests a higher likelihood of consciousness. Current AI systems score low on these checklists.  
* **Other Criteria:** Some propose lists of functional criteria like presence, imagination, attention, volition, emotion , or quantifiable tests based on semantic representation of self and reality (e.g., BSAST ). However, all current tests face challenges, including the difficulty of ensuring neutrality between theories and the persistent "audience problem"—how can we be sure observed behavior reflects genuine inner states rather than sophisticated mimicry?.

**Self-awareness**, as a component of consciousness, is also explored. Some propose it might emerge in embodied AI through the integration of sensory feedback loops, potentially simulating the function of brain regions like the insula involved in interoception and bodily awareness. This contrasts with purely computational views and emphasizes the role of embodiment. Self-awareness should be distinguished from Theory of Mind, which is about understanding *others'* minds.  
Regardless of the technical or philosophical reality, **public perception** of AI sentience is evolving rapidly. Surveys (like the AIMS survey) indicate surprisingly high and increasing levels of belief among the US public that some AI systems are already sentient (around 20% in 2023). There is significant moral concern for AI welfare (e.g., 71% agree sentient AI deserve respect) and support for legal rights (38%), alongside high levels of perceived threat and support for banning superintelligent or sentient AI (63-69%). The median forecast for sentient AI arrival was just five years in 2023\. This rapid shift in public opinion, potentially outpacing scientific consensus, creates significant societal and ethical challenges. The *perception* of sentience itself influences human-AI interaction, trust, and policy debates, irrespective of the AI's actual internal state. This underscores the ethical importance of responsible AI development and communication, leading to proposals for principles guiding research on potentially conscious AI, emphasizing caution, risk minimization, and transparency amidst uncertainty. The deep ambiguity surrounding AI consciousness, stemming from unresolved philosophical problems and inadequate empirical tests, makes definitive claims premature.

### **2.6 Benchmarking and Evaluation Challenges**

Evaluating the capabilities of increasingly complex AI systems, particularly those aiming for sophisticated cognitive functions, presents significant challenges that go beyond simple accuracy metrics. The field of AI evaluation is itself a dynamic area of research, grappling with how to meaningfully assess performance, understanding, reasoning, and safety.  
Established benchmarks like ImageNet (image classification), SQuAD (question answering), and SuperGLUE (language understanding) have driven progress but are now often "saturated," with top models achieving near-perfect or superhuman scores. This saturation makes it difficult to differentiate between models and may not reflect robust, generalizable capabilities. Consequently, researchers are developing **harder benchmarks** designed to probe deeper skills. Examples from 2023-2024 include SWE-bench (coding), HEIM (image generation), MMMU (general multimodal reasoning), MoCa (moral reasoning), AgentBench (agentic behavior), HaluEval (hallucinations), GPQA (graduate-level science questions), and PlanBench (planning). Benchmarks specifically targeting reasoning include BIG-Bench Hard, MuSR (Multi-Step Reasoning), and components of SuperGLUE like the Winograd Schema Challenge. TruthfulQA focuses on ethical reasoning and avoiding misinformation.  
A major concern with benchmark evaluation is **data contamination**, where test data inadvertently leaks into the training data, artificially inflating performance. This makes it difficult to assess true generalization. Efforts like AntiLeak-Bench aim to create contamination-free evaluation sets , and benchmarks like CausalProbe 2024 use fresh data to test reasoning beyond memorization.  
The rise of generative models has also led to an increased emphasis on **human evaluation**. Platforms like the Chatbot Arena Leaderboard rely on human preferences to rank models. While valuable for assessing subjective qualities like coherence or helpfulness, human evaluation is expensive, time-consuming, potentially subjective, and difficult to scale.  
A fundamental challenge lies in **operationalizing complex concepts like "understanding"** for evaluation. How can we measure if an LLM truly understands or is just exhibiting sophisticated pattern matching? Some frameworks attempt to dissociate different capabilities, such as distinguishing formal linguistic competence (grammar, syntax) from functional competence (using language to reason and interact with the world) , or breaking down capabilities into sub-steps like knowledge recall, utilization, and problem-solving. Evaluating these fine-grained skills requires more nuanced tasks and metrics.  
Evaluation needs to move **beyond accuracy** to encompass a wider range of desirable properties. Frameworks like FUTURE-AI emphasize criteria such as Fairness, Universality, Traceability, Usability, Robustness, and Explainability. Metrics should assess consistency across related tasks, the quality of justifications provided by reasoning models , robustness to perturbations, safety against misuse, and transparency.  
Finally, there is growing recognition of the **sociotechnical nature of benchmarks**. Benchmarks are not neutral, objective measures; they embody the values, assumptions, and goals of their creators and shape the direction of AI development. Different research communities may adopt different evaluation paradigms (e.g., focusing on performance vs. safety vs. cognitive fidelity), leading to fragmentation and potentially overlooked limitations. This highlights the need for critical reflection on *what* is being measured and *why*, and for greater transparency and standardization in evaluation practices. The evolution of benchmarks reflects AI progress but also the struggle to create tests that are robust, meaningful, and truly capture the complex capabilities associated with genuine thought. A multi-paradigm approach, combining quantitative metrics, qualitative analysis, human judgment, cognitive science insights, and sociotechnical awareness, appears necessary for a holistic assessment of advanced AI.

## **Part 3: Synthesis and Conclusion**

The preceding sections have traversed the complex terrain where continental philosophy intersects with the cutting edge of artificial intelligence research, particularly concerning the quest for capabilities resembling genuine thought. This concluding part synthesizes these threads, revisiting philosophical critiques in light of modern AI, identifying key convergences and divergences, and outlining potential future directions.

### **3.1 Revisiting Philosophical Critiques in Light of Modern AI**

The philosophical critiques, especially those rooted in phenomenology and existentialism, retain significant relevance even in the age of powerful LLMs and emerging AI architectures.

* **Dreyfus's Legacy:** Dreyfus's central critique focused on GOFAI's reliance on disembodied, context-free symbol manipulation, contrasting it with human intelligence as embodied, situated, skillful coping. While current LLMs operate differently from GOFAI, many remain fundamentally disembodied, learning from vast datasets of text and images but lacking direct, causal interaction with the physical world. Their successes in complex tasks challenge the *necessity* of human-like embodiment for *all* forms of high-level cognition. However, the persistent difficulties AI faces in areas requiring robust common sense, adaptability to novel physical situations, and solving the frame problem suggest that Dreyfus's emphasis on situatedness and embodiment remains pertinent. The rise of Embodied AI and VLA models can be seen as an implicit acknowledgment within AI research of the limitations of purely disembodied approaches, echoing Dreyfus's concerns. The exploration of neurodynamics as a potential substrate also finds echoes in modern connectionist approaches, though the specific mechanisms differ.  
* **Understanding and Grounding:** The debates surrounding LLM understanding directly engage with the issues raised by Searle, Gadamer, and Haugeland. While LLMs demonstrate impressive functional grounding (mastery of linguistic patterns) and developing social grounding (through interaction), their causal grounding remains weak and indirect. The argument that LLMs develop "world models" offers a potential rebuttal to the pure "syntax manipulation" charge of the CRA, suggesting emergent semantic organization. However, from a Gadamerian perspective, they still lack the historical situatedness and ethical depth for genuine dialogue. From Haugeland's existential viewpoint, they lack the "care" or "commitment" that underpins human understanding. Thus, while LLMs might achieve a *form* of understanding, it appears qualitatively different from human understanding rooted in lived experience and ontological commitment.  
* **Posthumanism and AI Agency:** Posthumanist frameworks provide tools to analyze AI systems like LLM agents not as deficient humans, but as novel socio-technical assemblages with unique properties and political entanglements. This perspective encourages moving beyond anthropocentric evaluation and focusing on the relational dynamics and emergent behaviors of AI within specific contexts, offering a different lens for ethical and safety considerations.

### **3.2 Convergences and Divergences**

Comparing the philosophical perspectives and technical advancements reveals areas of alignment and ongoing tension:

* **Convergence on Embodiment:** There is a clear convergence between the long-standing philosophical emphasis on embodiment (Heidegger, Merleau-Ponty, Dreyfus) and the rapidly growing field of Embodied AI. Both recognize the limitations of disembodied approaches and emphasize the role of interaction with the environment for developing robust intelligence and grounding.  
* **Tension on Reasoning/Logic:** Continental philosophy, particularly Dreyfus, highlighted the limits of formal logic and rule-following for capturing human intelligence. AI research, while acknowledging these limits, continues to push the boundaries of machine reasoning, developing sophisticated LLMs that perform complex logical, mathematical, and causal inference. Neuro-symbolic AI explicitly attempts to integrate formal reasoning with neural flexibility. The question remains whether these advanced reasoning capabilities represent genuine understanding or highly refined pattern matching, and whether they can overcome the brittleness observed in novel situations.  
* **Divergence on Consciousness:** While AI makes strides in simulating complex behaviors, including those associated with consciousness (e.g., self-reflection, reporting internal states), it remains far from addressing the philosophical "hard problem" of subjective experience (qualia). The gap between functional replication and genuine phenomenal consciousness persists, representing a major point of divergence between current AI capabilities and a core aspect of human mental life.

### **3.3 Future Directions: Towards AI with Genuine Thought?**

The intersection of continental philosophy and AI suggests several avenues for future research and development:

* **Integrating Philosophical Insights:** AI design and evaluation could benefit from more explicit engagement with concepts like situatedness, embodiment, temporality, and hermeneutic understanding. For instance, designing AI systems that model Haugeland's "care" or Gadamer's dialogical virtues could lead to more robust, trustworthy, and perhaps genuinely understanding systems, although how to operationalize these concepts remains a profound challenge. Posthumanist ethics offers alternative frameworks for safety and alignment beyond simple human control.  
* **Technical Frontiers:** Key technical challenges remain, including: scaling reasoning capabilities efficiently and reliably ; achieving robust generalization beyond training data, particularly for reasoning tasks ; solving the symbol grounding problem, especially causal grounding for LLMs ; developing more meaningful and robust evaluation benchmarks that capture deeper understanding ; ensuring the safety, ethics, and controllability of increasingly powerful AI systems ; and exploring the potential of alternative architectures (NeSy, Embodied, Memory-Augmented).  
* **Redefining "Thought":** The pursuit of Artificial General Intelligence (AGI) forces us to continually re-examine our definitions of "thought," "intelligence," and "understanding." Perhaps the goal should not be solely to replicate human cognition but to explore the diverse possibilities of intelligence, both biological and artificial. The journey towards advanced AI may ultimately teach us more about the nature of our own minds and the spectrum of possible cognitive architectures.

### **3.4 Concluding Remarks**

The dialogue between continental philosophy and artificial intelligence, particularly concerning the possibility of "genuine thought" in machines, is complex and far from resolved. Philosophical critiques rooted in phenomenology, existentialism, and hermeneutics continue to challenge the foundational assumptions of AI, emphasizing the importance of embodiment, situatedness, meaning, and understanding beyond mere computational prowess. Concurrently, AI research has achieved remarkable progress, developing LLMs capable of sophisticated reasoning, intent recognition, and self-improvement, alongside alternative architectures exploring neuro-symbolic integration, embodiment, and advanced memory systems.  
While current AI demonstrates impressive capabilities, significant gaps remain compared to the depth, robustness, and grounded nature of human cognition. The philosophical problem of consciousness and subjective experience remains largely untouched by technical advancements. Evaluating the true capabilities of AI beyond surface performance is an ongoing challenge, requiring more nuanced benchmarks and methodologies. Moving forward, a richer interplay between philosophical inquiry and technical innovation seems essential. Philosophy can help clarify the goals and assumptions of AI research, while AI advancements provide concrete testbeds for philosophical theories of mind and intelligence. The quest for artificial thought, whether ultimately successful in replicating human cognition or in creating novel forms of intelligence, promises to continue reshaping both our technology and our understanding of ourselves.  
**Table 2: Recent AI Advancements Towards Cognitive Capabilities (2023-2025)**

| Capability Area | Key Models/Frameworks | Core Innovation/Technique | Key Limitations/Challenges | Relevant Sources |
| :---- | :---- | :---- | :---- | :---- |
| **System 2 Reasoning** | OpenAI o1/o3, DeepSeek-R1, G$^2$-Reasoner | Native CoT, RL w/ PRM/ORM, MCTS/Search, Step-by-step verification/refinement, Goal-oriented prompting, Knowledge integration | Brittleness, Level-1 vs Level-2 reasoning gap (esp. causal), Scalability/Efficiency, Generalization, Hidden internal steps | \---- |
| **Intent Understanding / ToM** | LLMs in CIS, SetFit (vs LLM), Thought-tracing, BDI models | Context tracking, Query clarification/reformulation, User simulation, Intent classification (ICL/CoT), Bayesian inference simulation | Deep state inference, Evaluation challenges (DoM, contamination, shortcuts), Robustness, Scalability of simulation | \---- |
| **Self-Improvement** | LADDER (+TTRL), STaR, STL, AutoAgents | Recursive problem decomposition, Self-verification, RL on self-generated curriculum, Bootstrapped value models, Multi-agent teams | Reliance on verifiable domains/reward signals, Exploration efficiency (esp. small models), Scalability | \-- |
| **Embodied AI / Grounding** | VLA Models (RT-2, OpenVLA, etc.), World Models (Dreamer, WorldCoder), Cosmos-Reason1, Animal-AI Env. | Vision-Language-Action fusion, Dynamics learning, World simulation (programmatic/visual), Physical common sense ontology, Task/Sequential grounding | Sim-to-real gap, Data scarcity/diversity, Safety, Real-time constraints, Robust physical reasoning, Language grounding complexity | \----- |
| **Neuro-Symbolic AI** | LNUs, SatNet, Systems integrating KGs/Rules | Embedding differentiable logic, Combining neural perception & symbolic reasoning, Formal explanations | Scalability, Integration complexity, Learning symbolic knowledge, Interpretability of hybrid systems | \-- |
| **Memory-Augmented AI** | EM-LLM, MemReasoner | Episodic event segmentation (surprise-based), Two-stage retrieval (similarity+contiguity), Temporal order learning, Memory hopping | Efficient management of large memory stores, Optimal retrieval strategies, Integration with reasoning processes | \- |
| **Consciousness / Qualia** | AIMS Survey, ACT/Chip Test, Theory Checklists | Evaluating public perception, Proposing theoretical/behavioral tests | Lack of consensus definition, Hard problem of qualia, Test neutrality/reliability, Simulation vs. reality, Audience problem, Ethical implications | \-- |

#### **Works cited**

1\. Philosophy of Artificial Intelligence \- Bibliography \- PhilPapers, https://philpapers.org/browse/philosophy-of-artificial-intelligence 2\. Artificial Intelligence \- Stanford Encyclopedia of Philosophy, https://plato.stanford.edu/entries/artificial-intelligence/ 3\. Artificial general intelligence \- Wikipedia, https://en.wikipedia.org/wiki/Artificial\_general\_intelligence 4\. Haugeland's understanding: on artificial intelligence and existential ..., https://www.researchgate.net/publication/389876972\_Haugeland's\_understanding\_on\_artificial\_intelligence\_and\_existential\_ontology/download 5\. Overview of Chatbots with special emphasis on artificial intelligence-enabled ChatGPT in medical science, https://pmc.ncbi.nlm.nih.gov/articles/PMC10644239/ 6\. From System 1 to System 2: A Survey of Reasoning Large Language Models \- arXiv, https://arxiv.org/pdf/2502.17419? 7\. Generative AI's Act o1: The Reasoning Era Begins | Sequoia Capital, https://www.sequoiacap.com/article/generative-ais-act-o1/ 8\. cspeech.ucd.ie, https://cspeech.ucd.ie/Fred/docs/WhyHeideggerianAIFailed.pdf 9\. Philosophy Eats AI \- MIT Sloan Management Review, https://sloanreview.mit.edu/article/philosophy-eats-ai/ 10\. Logical Reasoning in Large Language Models: A Survey \- arXiv, https://arxiv.org/html/2502.09100v1 11\. arxiv.org, http://arxiv.org/pdf/2407.09450 12\. Learning to reason with LLMs | OpenAI, https://openai.com/index/learning-to-reason-with-llms/ 13\. AI Consciousness: A Philosophical Exploration \- Reddit, https://www.reddit.com/r/consciousness/comments/1hd1w8o/ai\_consciousness\_a\_philosophical\_exploration/ 14\. Critique of some recent philosophy of LLMs' minds \- AI Alignment Forum, https://www.alignmentforum.org/posts/ejEgaYSaefCevapPa/critique-of-some-recent-philosophy-of-llms-minds 15\. A Framework for the Foundation of the Philosophy of Artificial Intelligence \- Digital Commons@Lindenwood University, https://digitalcommons.lindenwood.edu/cgi/viewcontent.cgi?article=1682\&context=faculty-research-papers 16\. Hubert Dreyfus discusses Heidegger & Merleau-Ponty \- YouTube, https://www.youtube.com/watch?v=Oq4sjz38z5A 17\. Dreyfus on Heidegger, Merleau-Ponty, and Artificial Intelligence | The Partially Examined Life Philosophy Podcast, https://partiallyexaminedlife.com/2011/12/19/dreyfus-on-heidegger-merleau-ponty-and-artificial-intelligence/ 18\. Hubert Dreyfus, preeminent philosopher and AI critic, dies at 87 \- Berkeley News, https://news.berkeley.edu/2017/04/24/hubert-dreyfus/ 19\. (PDF) Framing the predictive mind: why we should think again about Dreyfus \- ResearchGate, https://www.researchgate.net/publication/380362840\_Framing\_the\_predictive\_mind\_why\_we\_should\_think\_again\_about\_Dreyfus 20\. Hubert L. Dreyfus's Critique of Classical AI and its Rationalist Assumptions \- ResearchGate, https://www.researchgate.net/publication/225233726\_Hubert\_L\_Dreyfus's\_Critique\_of\_Classical\_AI\_and\_its\_Rationalist\_Assumptions 21\. Why Heideggerian AI Failed and how Fixing it would Require making it more, https://leidlmair.at/doc/WhyHeideggerianAIFailed.pdf 22\. Why Heideggerian AI Failed and How Fixing it Would Require Making it More Heideggerian, https://www.tandfonline.com/doi/abs/10.1080/09515080701239510 23\. Cognition in Context: Phenomenology, Situated Robotics and the Frame Problem | Request PDF \- ResearchGate, https://www.researchgate.net/publication/37244769\_Cognition\_in\_Context\_Phenomenology\_Situated\_Robotics\_and\_the\_Frame\_Problem 24\. technophany.philosophyandtechnology.network, https://technophany.philosophyandtechnology.network/article/download/18134/20942/43090 25\. Continental Philosophy, Miscellaneous \- Bibliography \- PhilPapers, https://philpapers.org/browse/continental-philosophy-miscellaneous 26\. 2\. Towards a Hermeneutic Phenomenology of Technology \- Open Book Publishers, https://books.openbookpublishers.com/10.11647/obp.0421/ch2.xhtml 27\. Haugeland's understanding: on artificial intelligence and existential ontology. \- PhilPapers, https://philpapers.org/rec/LEMHUO 28\. Haugeland's understanding: on artificial intelligence and existential ontology \- PhilPapers, https://philpapers.org/rec/LEMHUO-2 29\. Avery Rijos, Posthumanist Phenomenology and Artificial Intelligence ..., https://philarchive.org/rec/RIJAPA-3 30\. The Chinese Room \- Bibliography \- PhilPapers, https://philpapers.org/browse/the-chinese-room 31\. Chinese Room Argument | Internet Encyclopedia of Philosophy, https://iep.utm.edu/chinese-room-argument/ 32\. Robert I. Damper, The logic of Searle's Chinese room argument \- PhilPapers, https://philpapers.org/rec/DAMTLO 33\. Chinese room \- Wikipedia, https://en.wikipedia.org/wiki/Chinese\_room 34\. Full article: LLMs, Turing tests and Chinese rooms: the prospects for meaning in large language models \- Taylor & Francis Online, https://www.tandfonline.com/doi/full/10.1080/0020174X.2024.2446241 35\. On the Creativity of Large Language Models \- arXiv, https://arxiv.org/html/2304.00008v5 36\. Technical Performance \- AI Index, https://aiindex.stanford.edu/wp-content/uploads/2024/04/HAI\_AI-Index-Report-2024\_Chapter2.pdf 37\. MACHINES OF MEANING \- arXiv, https://arxiv.org/html/2412.07975v1 38\. A Survey on Vision-Language-Action Models for Embodied AI \- arXiv, https://arxiv.org/html/2405.14093v4 39\. Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning \- arXiv, https://arxiv.org/html/2503.15558 40\. Exploring Embodied Multimodal Large Models: Development, Datasets, and Future Directions \- arXiv, https://arxiv.org/html/2502.15336v1 41\. A Call for Embodied AI \- arXiv, https://arxiv.org/html/2402.03824v3 42\. SpLU-RoboNLP 2024, https://splu-robonlp-2024.github.io/ 43\. What is Grounding in AI and What are the Best Techniques? \- Open Data Science, https://opendatascience.com/what-is-grounding-in-ai-and-what-are-the-best-techniques/ 44\. Stop treating 'AGI' as the north-star goal of AI research \- arXiv, https://arxiv.org/html/2502.03689v2 45\. System 2 Reasoning Capabilities Are Nigh: A New Frontier in AI Cognition \- Indika AI, https://www.indikaai.com/blog/system-2-reasoning-capabilities-are-nigh-a-new-frontier-in-ai-cognition 46\. (PDF) System 2 Reasoning Capabilities Are Nigh \- ResearchGate, https://www.researchgate.net/publication/384680342\_System\_2\_reasoning\_capabilities\_are\_nigh 47\. NeurIPS Poster Unveiling Causal Reasoning in Large Language ..., https://neurips.cc/virtual/2024/poster/96872 48\. A Tutorial on LLM Reasoning: Relevant Methods behind ChatGPT o1 \- arXiv, https://arxiv.org/html/2502.10867v1 49\. Computational Thinking- Beyond Data Scaling – Expanding AI's Cognitive Horizon, https://www.dvj-insights.com/computational-thinking-beyond-data-scaling-expanding-ais-cognitive-horizon/ 50\. Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging \- arXiv, https://arxiv.org/html/2503.20641v1 51\. A Survey of Efficient Reasoning for Large Reasoning Models: Language, Multimodality, and Beyond \- arXiv, https://arxiv.org/html/2503.21614v1 52\. arXiv:2402.12091v1 \[cs.CL\] 19 Feb 2024, https://zeynepaltan.info/LLMandLogicorMimick.pdf 53\. Unveiling Causal Reasoning in Large Language Models: Reality or Mirage? \- NIPS papers, https://proceedings.neurips.cc/paper\_files/paper/2024/file/af2bb2b2280d36f8842e440b4e275152-Paper-Conference.pdf 54\. Reasoning Language Models: A Blueprint \- Scalable Parallel Computing Lab, https://spcl.inf.ethz.ch/Publications/.pdf/besta-reasoning.pdf 55\. Self-Distilling DeepSeek-R1: Accelerating Reasoning with Turbo Speculation for 2x Faster Inference \- Predibase, https://predibase.com/blog/predibase.com/blog/deepseek-r1-self-distillation-turbo-speculation 56\. DeepSeek V3 vs R1: A Guide With Examples \- DataCamp, https://www.datacamp.com/blog/deepseek-r1-vs-v3 57\. DeepSeek R1: Reshaping AI Reasoning Models \- SmythOS, https://smythos.com/ai-agents/agent-architectures/deepseek-r1/ 58\. AICoP: Reasoning at Scale \- From GPT to DeepSeek | Emerging Technologies, https://etc.cuit.columbia.edu/news/aicop-reasoning-scale-gpt-deepseek 59\. Let's think step by step: Chain of Thought prompting in LLMs \- Keywords AI, https://www.keywordsai.co/blog/lets-think-step-by-step-chain-of-thought-prompting-in-llms 60\. Self-Verification Prompting: Enhancing LLM Accuracy in Reasoning Tasks, https://learnprompting.org/docs/advanced/self\_criticism/self\_verification 61\. Exploring Reasoning LLMs and Their Real-World Applications \- GetStream.io, https://getstream.io/blog/reasoning-llms/ 62\. Real-time computing: why "thinking" AI models are shaping the future | Silamir Group, https://www.silamir.com/en/insight/test-time-compute-why-thinking-ai-models-are-shaping-the-future/ 63\. NOT ALL LLM REASONERS ARE CREATED EQUAL \- OpenReview, https://openreview.net/pdf?id=T8PzwgYgmn 64\. Theory Is All You Need: AI, Human Cognition, and Causal Reasoning† \- Boston University, https://www.bu.edu/dbi/files/2024/08/FelinHolwegAug2024\_SSRN.pdf 65\. With OpenAI o1 & o3 Models Devs Can Set Reasoning Effort | by Cobus Greyling | Medium, https://cobusgreyling.medium.com/with-openai-o1-o3-models-devs-can-set-reasoning-effort-b05907c6dfa1 66\. Deepseek R1 is slow\!? : r/AI\_Agents \- Reddit, https://www.reddit.com/r/AI\_Agents/comments/1i78uzo/deepseek\_r1\_is\_slow/ 67\. \[2502.09100\] Logical Reasoning in Large Language Models: A Survey \- arXiv, https://arxiv.org/abs/2502.09100 68\. Paper page \- Logical Reasoning in Large Language Models: A Survey \- Hugging Face, https://huggingface.co/papers/2502.09100 69\. José A. Alonso: "Logical reasoning in large language models: A survey. \~ Hanmeng Liu et als. arxiv.org/abs/2502.09100 \#LLMs \#Logic \#Reasoning" — Bluesky, https://bsky.app/profile/jalonso.bsky.social/post/3libsuhnw322h 70\. \[2502.03671\] Advancing Reasoning in Large Language Models: Promising Methods and Approaches \- arXiv, https://arxiv.org/abs/2502.03671 71\. Improving Causal Reasoning in Large Language Models: A Survey \- arXiv, https://arxiv.org/html/2410.16676v1 72\. A Survey on Enhancing Causal Reasoning Ability of Large Language Models, https://www.researchgate.net/publication/389786349\_A\_Survey\_on\_Enhancing\_Causal\_Reasoning\_Ability\_of\_Large\_Language\_Models/download 73\. What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models \- arXiv, https://arxiv.org/html/2310.06627 74\. Counterfactual Causal Inference in Natural Language with Large Language Models \- arXiv, https://arxiv.org/html/2410.06392v1 75\. Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?, https://proceedings.neurips.cc/paper\_files/paper/2024/file/dfaa29ed28dfa175bcc5e2a54aa199f8-Paper-Conference.pdf 76\. Using LLMs to Investigate Correlations of Conversational Follow-up Queries with User Satisfaction \- arXiv, https://arxiv.org/html/2407.13166v1 77\. \[2201.08808\] Conversational Information Seeking \- arXiv, https://arxiv.org/abs/2201.08808 78\. Query Understanding in LLM-based Conversational Information Seeking \- arXiv, https://arxiv.org/html/2504.06356v1 79\. \[2504.06356\] Query Understanding in LLM-based Conversational Information Seeking \- arXiv, https://arxiv.org/abs/2504.06356 80\. Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, https://aclanthology.org/volumes/2024.emnlp-main/ 81\. Full article: Enhancing Intent Classifier Training with Large Language Model-generated Data \- Taylor and Francis, https://www.tandfonline.com/doi/full/10.1080/08839514.2024.2414483 82\. Intent Detection in the Age of LLMs \- ACL Anthology, https://aclanthology.org/2024.emnlp-industry.114.pdf 83\. MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations | OpenReview, https://openreview.net/forum?id=nY9nITZQjc 84\. Preferences for AI Explanations Based on Cognitive Style and Socio-Cultural Factors \- King's College London Research Portal, https://kclpure.kcl.ac.uk/portal/files/251959362/CSCW\_CameraReady\_Study1\_authormanuscript.pdf 85\. Understanding BDI Agents in Agent-Oriented Programming \- SmythOS, https://smythos.com/ai-agents/agent-architectures/agent-oriented-programming-and-bdi-agents/ 86\. Belief-Desire-Intention Model: BDI Definition | Vaia, https://www.vaia.com/en-us/explanations/engineering/artificial-intelligence-engineering/belief-desire-intention-model/ 87\. Propositional Interpretability in Artificial Intelligence \- arXiv, https://arxiv.org/pdf/2501.15740? 88\. Understanding Human-Centred AI: a review of its defining elements and a research agenda, https://www.tandfonline.com/doi/full/10.1080/0144929X.2024.2448719?src= 89\. Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models \- arXiv, https://arxiv.org/html/2502.11881 90\. Mind Your Theory: Theory of Mind Goes Deeper Than Reasoning \- arXiv, https://arxiv.org/html/2412.13631v1 91\. ToM-agent: Large Language Models as Theory of Mind Aware Generative Agents with Counterfactual Reflection \- arXiv, https://arxiv.org/html/2501.15355v1 92\. The Evolving Constellation of Theory of Mind in Artificial Intelligence | by Carlos E. Perez | Intuition Machine | Feb, 2025 | Medium, https://medium.com/intuitionmachine/the-evolving-constellation-of-theory-of-mind-in-artificial-intelligence-604863f57917 93\. Spontaneous Theory of Mind for Artificial Intelligence \- arXiv, https://arxiv.org/html/2402.13272v1 94\. Evaluating large language models in theory of mind tasks \- PMC \- PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC11551352/ 95\. Evaluating large language models in theory of mind tasks \- PubMed, https://pubmed.ncbi.nlm.nih.gov/39471222/ 96\. \[2502.21098\] Re-evaluating Theory of Mind evaluation in large language models \- arXiv, https://arxiv.org/abs/2502.21098 97\. \[2412.13631\] Mind Your Theory: Theory of Mind Goes Deeper Than Reasoning \- arXiv, https://arxiv.org/abs/2412.13631 98\. \[Literature Review\] Mind Your Theory: Theory of Mind Goes Deeper Than Reasoning, https://www.themoonlight.io/review/mind-your-theory-theory-of-mind-goes-deeper-than-reasoning 99\. Mind Your Theory: Theory of Mind Goes Deeper Than Reasoning \- ResearchGate, https://www.researchgate.net/publication/387184083\_Mind\_Your\_Theory\_Theory\_of\_Mind\_Goes\_Deeper\_Than\_Reasoning 100\. A Systematic Review on the Evaluation of Large Language Models in Theory of Mind Tasks, https://arxiv.org/html/2502.08796v1 101\. \[2502.11881\] Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models, https://arxiv.org/abs/2502.11881 102\. Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models \- ResearchGate, https://www.researchgate.net/publication/389091436\_Hypothesis-Driven\_Theory-of-Mind\_Reasoning\_for\_Large\_Language\_Models 103\. Publications | Xuan (Tan Zhi Xuan), https://ztangent.github.io/publications/ 104\. Theory of Mind Imitation by LLMs for Physician-Like Human Evaluation \- medRxiv, https://www.medrxiv.org/content/10.1101/2025.03.01.25323142v1.full.pdf 105\. Large Language Models for Mental Health Applications: Systematic Review, https://mental.jmir.org/2024/1/e57400 106\. Exploring the Credibility of Large Language Models for Mental Health Support: Protocol for a Scoping Review, https://pmc.ncbi.nlm.nih.gov/articles/PMC11822324/ 107\. Integrating large language models in mental health practice: a qualitative descriptive study based on expert interviews \- Frontiers, https://www.frontiersin.org/journals/public-health/articles/10.3389/fpubh.2024.1475867/full 108\. Large Language Models for Mental Health Applications: Systematic Review \- PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC11530718/ 109\. Professional Agents \- Evolving Large Language Models into Autonomous Experts with Human-Level Competencies \- arXiv, https://arxiv.org/html/2402.03628v1 110\. LADDER: Self-Improving LLMs Through Recursive Problem Decomposition \- arXiv, https://arxiv.org/html/2503.00735v3 111\. Language Models can Self-Improve at State-Value Estimation for Better Search \- arXiv, https://arxiv.org/html/2503.02878v1 112\. \[Literature Review\] LADDER: Self-Improving LLMs Through Recursive Problem Decomposition \- Moonlight, https://www.themoonlight.io/review/ladder-self-improving-llms-through-recursive-problem-decomposition 113\. LADDER: Self-Improving LLMs Through Recursive Problem Decomposition \- ResearchGate, https://www.researchgate.net/publication/389548458\_LADDER\_Self-Improving\_LLMs\_Through\_Recursive\_Problem\_Decomposition 114\. Reasoning Under 1 Billion: Memory-Augmented Reinforcement Learning for Large Language Models \- arXiv, https://arxiv.org/html/2504.02273v1 115\. General Reasoning Requires Learning to Reason from the Get-go \- arXiv, https://arxiv.org/html/2502.19402v1 116\. Large Language Models Can Self-Improve in Long-context Reasoning \- arXiv, https://arxiv.org/html/2411.08147v1 117\. Standard Neural Computation Alone Is Insufficient for Logical Intelligence \- arXiv, https://arxiv.org/html/2502.02135v1 118\. Standard Neural Computation Alone Is Insufficient for Logical Intelligence \- Powerdrill, https://powerdrill.ai/discover/summary-standard-neural-computation-alone-is-insufficient-cm6se9bzx9cvx07s315zh1tq9 119\. Standard Neural Computation Alone Is Insufficient for Logical Intelligence \- AIModels.fyi, https://www.aimodels.fyi/papers/arxiv/standard-neural-computation-alone-is-insufficient-logical 120\. Neuro-Symbolic Artificial Intelligence | springerprofessional.de, https://www.springerprofessional.de/neuro-symbolic-artificial-intelligence/50391468 121\. (PDF) Formal Explanations for Neuro-Symbolic AI \- ResearchGate, https://www.researchgate.net/publication/385091461\_Formal\_Explanations\_for\_Neuro-Symbolic\_AI 122\. Neuro-Symbolic AI | Data | Paperback \- Packt, https://www.packtpub.com/en-cl/product/neuro-symbolic-ai-9781804617625 123\. Natural Language Processing and Neurosymbolic AI: The Role of Neural Networks with Knowledge-Guided Symbolic Approaches \- Digital Commons@Lindenwood University, https://digitalcommons.lindenwood.edu/cgi/viewcontent.cgi?article=1610\&context=faculty-research-papers 124\. Neurosymbolic AI for Enhancing Instructability in Generative AI \- arXiv, https://arxiv.org/html/2407.18722v1 125\. citeseerx.ist.psu.edu, https://citeseerx.ist.psu.edu/document?repid=rep1\&type=pdf\&doi=db71d94cd939c674549cac34b2e28e400f406957 126\. Minds in movement: embodied cognition in the age of artificial intelligence \- Journals, https://royalsocietypublishing.org/doi/10.1098/rstb.2023.0144 127\. \[2405.14093\] A Survey on Vision-Language-Action Models for Embodied AI \- arXiv, https://arxiv.org/abs/2405.14093 128\. A Survey On Vision-Language-Action Models For Embodied AI | PDF \- Scribd, https://www.scribd.com/document/737885773/2405-14093v1 129\. A Survey on Vision-Language-Action Models for Embodied AI \- ResearchGate, https://www.researchgate.net/publication/380820805\_A\_Survey\_on\_Vision-Language-Action\_Models\_for\_Embodied\_AI?\_share=1 130\. UpcomAI/Embodied-AI-papers \- GitHub, https://github.com/UpcomAI/Embodied-AI-papers 131\. Evaluating World Models with LLM for Decision Making \- arXiv, https://arxiv.org/html/2411.08794v1 132\. Compositional World Models for Embodied Multi-Agent Cooperation \- Tianmin Shu, https://www.tshu.io/publications.html 133\. proceedings.neurips.cc, https://proceedings.neurips.cc/paper\_files/paper/2024/file/820c61a0cd419163ccbd2c33b268816e-Paper-Conference.pdf 134\. zchoi/Awesome-Embodied-Robotics-and-Agent: This is a curated list of "Embodied AI or robot with Large Language Models" research. Watch this repository for the latest updates\! \- GitHub, https://github.com/zchoi/Awesome-Embodied-Robotics-and-Agent 135\. From Screens to Scenes: A Survey of Embodied AI in Healthcare \- ResearchGate, https://www.researchgate.net/publication/387976351\_From\_Screens\_to\_Scenes\_A\_Survey\_of\_Embodied\_AI\_in\_Healthcare 136\. Task-oriented Sequential Grounding in 3D Scenes \- Tengyu Liu, https://tengyu.ai/assets/pdf/preprint-SequentialGrounding.pdf 137\. GRASP: A Novel Benchmark for Evaluating Language GRounding and Situated Physics Understanding in Multimodal Language Models \- IJCAI, https://www.ijcai.org/proceedings/2024/0696.pdf 138\. \[2503.15558\] Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning \- arXiv, https://arxiv.org/abs/2503.15558 139\. Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning \- Hugging Face, https://huggingface.co/papers/2503.15558 140\. Investigating the physical common-sense of LLMs in a 3D embodied environment \- arXiv, https://arxiv.org/html/2410.23242v1 141\. \[2410.23242\] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment \- arXiv, https://arxiv.org/abs/2410.23242 142\. AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and Symbolic Reasoning \- ResearchGate, https://www.researchgate.net/publication/390142733\_AlphaSpace\_Enabling\_Robotic\_Actions\_through\_Semantic\_Tokenization\_and\_Symbolic\_Reasoning 143\. A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment \- ResearchGate, https://www.researchgate.net/publication/385387058\_A\_little\_less\_conversation\_a\_little\_more\_action\_please\_Investigating\_the\_physical\_common-sense\_of\_LLMs\_in\_a\_3D\_embodied\_environment 144\. The Animal-AI Environment: A virtual laboratory for comparative cognition and artificial intelligence research \- PMC \- PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC11870899/ 145\. Neural Force Field: Learning Generalized Physical Representation from a Few Examples, https://arxiv.org/html/2502.08987v1 146\. AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and Symbolic Reasoning \- arXiv, https://arxiv.org/pdf/2503.18769 147\. Cognitive Science-Inspired Evaluation of Core Capabilities for Object Understanding in AI \- arXiv, https://arxiv.org/pdf/2503.21668? 148\. Digital twins to embodied artificial intelligence: review and perspective \- OAE Publishing Inc., https://www.oaepublish.com/articles/ir.2025.11?to=comment 149\. Full article: How does technology-based embodied learning affect learning effectiveness? – Based on a systematic literature review and meta-analytic approach, https://www.tandfonline.com/doi/full/10.1080/10494820.2025.2479176 150\. Towards Self-Aware AI: Embodiment, Feedback Loops, and the ..., https://www.preprints.org/manuscript/202411.0661/v1 151\. (PDF) Memory Architectures in Long-Term AI Agents: Beyond Simple State Representation, https://www.researchgate.net/publication/388144017\_Memory\_Architectures\_in\_Long-Term\_AI\_Agents\_Beyond\_Simple\_State\_Representation 152\. Can Memory-Augmented Language Models Generalize on Reasoning-in-a-Haystack Tasks? \- arXiv, https://arxiv.org/html/2503.07903v1 153\. Neural networks that overcome classic challenges through practice \- arXiv, https://arxiv.org/pdf/2410.10596 154\. NeurIPS Poster ViLCo-Bench: VIdeo Language COntinual learning Benchmark, https://neurips.cc/virtual/2024/poster/97567 155\. Neuro-Symbolic AI in 2024: A Systematic Review \- arXiv, https://arxiv.org/html/2501.05435v1 156\. Cognitive Agent Architectures: Revolutionizing AI with Intelligent Decision-Making Systems, https://smythos.com/ai-agents/agent-architectures/cognitive-agent-architectures/ 157\. arXiv:2403.19669v2 \[cs.LG\] 10 May 2024 \- Computational Cognitive Science Lab, https://cocosci.princeton.edu/papers/chen2024analyze.pdf 158\. From Prompting to Thinking \-The Cognitive Engine | by Bijit Ghosh | Feb, 2025 | Medium, https://medium.com/@bijit211987/from-prompting-to-thinking-the-cognitive-engine-60f8d0cb5d54 159\. Beyond Input-Output Reasoning: Four Key Properties of Cognitive AI \- Medium, https://medium.com/towards-data-science/beyond-input-output-reasoning-four-key-properties-of-cognitive-ai-3f82cde8cf1e 160\. Cognitive AI Explained: Impact and Future in the Digital World | Vation Ventures Research, https://www.vationventures.com/research-article/cognitive-ai-explained-impact-and-future-in-the-digital-world 161\. THE COGNITIVE LEAP, https://f.hubspotusercontent10.net/hubfs/8473011/Landing%20Pages%20Materials/BeyondLimits\_CognitiveAI\_WhitePaper\_8.20.pdf 162\. Towards Conscious Service Robots \- arXiv, https://arxiv.org/html/2501.15198v1 163\. Can AI Ever Become Conscious? \- Article (Preprint v1) by Ashkan Farhadi | Qeios, https://www.qeios.com/read/UJAHLZ 164\. conscium.com, https://conscium.com/wp-content/uploads/2024/11/Principles-for-Conscious-AI.pdf 165\. Consciousness, Qualia, and AI: Can We Build What We Don't ..., https://medium.com/@adnanmasood/consciousness-qualia-and-ai-can-we-build-what-we-dont-understand-3de185008ffe 166\. AI and Philosophy: Exploring Intelligence, Consciousness, and Ethics, https://www.cognitech.systems/blog/artificial-intelligence/entry/ai-philosophy 167\. "Qualia Control" in Large Language Models | Psychology Today, https://www.psychologytoday.com/us/blog/the-digital-self/202403/qualia-control-in-large-language-models 168\. The Philosophy of AI and the Challenge of Qualia : r/consciousness \- Reddit, https://www.reddit.com/r/consciousness/comments/1g27gg5/the\_philosophy\_of\_ai\_and\_the\_challenge\_of\_qualia/ 169\. 4 Types of AI: Getting to Know Artificial Intelligence \- Coursera, https://www.coursera.org/articles/types-of-ai 170\. How Will We Know if AI Becomes Conscious? | American Brain Foundation, https://www.americanbrainfoundation.org/how-will-we-know-if-ai-becomes-conscious/ 171\. Turing test \- Wikipedia, https://en.wikipedia.org/wiki/Turing\_test 172\. Susan Schneider's Proposed Tests for AI Consciousness: Promising but Flawed \- Information Technology Solutions, https://www.faculty.ucr.edu/\~eschwitz/SchwitzPapers/SchneiderCrit-200828.pdf 173\. This Test for Machine Consciousness Has an Audience Problem \- Nautilus Magazine, https://nautil.us/this-test-for-machine-consciousness-has-an-audience-problem-237652/ 174\. (PDF) Reviewing Tests for Machine Consciousness \- ResearchGate, https://www.researchgate.net/publication/325498266\_Reviewing\_Tests\_for\_Machine\_Consciousness 175\. I have Created a Quantifiable Test for AI Self-Awareness \- OpenAI Developer Community, https://community.openai.com/t/i-have-created-a-quantifiable-test-for-ai-self-awareness/28234 176\. Perceptions of Sentient AI and Other Digital Minds: Evidence from the AI, Morality, and Sentience (AIMS) Survey \- arXiv, https://arxiv.org/html/2407.08867v3 177\. What Do People Think about Sentient AI? \- arXiv, https://arxiv.org/html/2407.08867v1 178\. Perceptions of Sentient AI and Other Digital Minds: Evidence from the AI, Morality, and Sentience (AIMS) Survey | Jacy Reese Anthis, Janet V.T. Pauketat, Ali Ladak, Aikaterina Manoli \- Reddit, https://www.reddit.com/r/Sentientism/comments/1j8wzil/perceptions\_of\_sentient\_ai\_and\_other\_digital/ 179\. Ascribing consciousness to artificial intelligence: human-AI interaction and its carry-over effects on human-human interaction \- Frontiers, https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1322781/full 180\. Best Benchmarks for Evaluating LLMs' Critical Thinking Abilities \- Galileo AI, https://www.galileo.ai/blog/best-benchmarks-for-evaluating-llms-critical-thinking-abilities 181\. Publications | Liangming Pan, https://liangmingpan.bio/publications/ 182\. Can We Trust AI Benchmarks? An Interdisciplinary Review of Current Issues in AI Evaluation, https://arxiv.org/html/2502.06559v1 183\. Benchmarking Large Language Models in Retrieval-Augmented Generation | Proceedings of the AAAI Conference on Artificial Intelligence, https://ojs.aaai.org/index.php/AAAI/article/view/29728 184\. Most Influential ArXiv (Artificial Intelligence) Papers (2024-10) \- Paper Digest, https://www.paperdigest.org/2024/10/most-influential-arxiv-artificial-intelligence-papers-2024-10/ 185\. arXiv:2502.15620v1 \[cs.AI\] 21 Feb 2025, https://arxiv.org/pdf/2502.15620? 186\. Using large language models in psychology \- NSF Public Access Repository, https://par.nsf.gov/servlets/purl/10575508 187\. FAC²E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition \- arXiv, https://arxiv.org/html/2403.00126v1 188\. Dissociating language and thought in large language models \- MIT, https://web.mit.edu/bcs/nklab/media/pdfs/Mahowald.TICs2024.pdf 189\. FUTURE-AI: international consensus guideline for trustworthy and deployable artificial intelligence in healthcare | The BMJ, https://www.bmj.com/content/388/bmj-2024-081554 190\. AI Benchmarks and Datasets for LLM Evaluation \- arXiv, https://arxiv.org/html/2412.01020v1 191\. Can Large Language Models Transform Computational Social Science? \- MIT Press Direct, https://direct.mit.edu/coli/article/50/1/237/118498/Can-Large-Language-Models-Transform-Computational 192\. A Survey of Efficient Reasoning for Large Reasoning Models: Language, Multimodality, and Beyond \- ResearchGate, https://www.researchgate.net/publication/390248127\_A\_Survey\_of\_Efficient\_Reasoning\_for\_Large\_Reasoning\_Models\_Language\_Multimodality\_and\_Beyond